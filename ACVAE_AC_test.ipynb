{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drive and git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxjrZNSbCVzE",
        "outputId": "f263b940-64fc-4681-9207-08cd4c897dfd"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS_baNFJ-NTa",
        "outputId": "2eb6609d-e057-4b91-bb4b-9b87209c282b"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    !git clone https://torresbf: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@github.com/torresbf/acvae-vc.git\n",
        "    !git config --global user.email \"beftorres@hotmail.com\"\n",
        "    !git config --global user.name \"Bernardo\"\n",
        "\n",
        "    #!git clone https://github.com/torresbf/acvae-vc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXVu6NY9Vp0-",
        "outputId": "e105f388-3253-4422-e761-2aa4dd60ae07"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    %cd \"/content/acvae-vc/\"\n",
        "    !git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6GfhHxBWRtr"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJj80qc-XRY9",
        "outputId": "975b8c1b-0985-4e0b-98bc-8c2b94e53191"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !pip install pyworld pysptk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsJF-YyEVnR6",
        "outputId": "b11b9b9b-d750-49e8-990a-68a5c7697f55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import os\n",
        "import pyworld\n",
        "import librosa\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from preprocess import *\n",
        "from model import ACVAE\n",
        "from data import data_load, data_load_preprocessed\n",
        "from utils import *\n",
        "import soundfile as sf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ljuAmOWU2N"
      },
      "source": [
        "## Data Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KWyhs935W0Ap"
      },
      "outputs": [],
      "source": [
        "model_name = \"model_lambda70_f2f3m1m2\"\n",
        "model_dir = \"./model/\" + model_name\n",
        "\n",
        "data_dir = \"./data/vcc2016_training/\"\n",
        "voice_dir_list = [\"SF1\", \"SF2\", \"SM1\", \"SM2\"]\n",
        "\n",
        "data_npy_dir = \"./data/vcc2016_npy/\"\n",
        "\n",
        "output_dir = \"./converted_voices/test/\" + model_name + \"_training_progress\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-wlD9CjWhYW"
      },
      "source": [
        "## Model Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampling_rate = 16000\n",
        "num_mcep = 36\n",
        "frame_period = 5.0\n",
        "n_frames = 1024 \n",
        "\n",
        "lambda_p = 70\n",
        "lambda_s = 70\n",
        "nb_label = len(voice_dir_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VtLKkKCYX_hz"
      },
      "outputs": [],
      "source": [
        "def conv_all(model):\n",
        "    print(\"Conversion Start.\")\n",
        "    \n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    \n",
        "    for s_label in range(nb_label):\n",
        "        \n",
        "        output_label_dir = os.path.join(output_dir, voice_dir_list[s_label])\n",
        "        if not os.path.exists(output_label_dir):\n",
        "            os.makedirs(output_label_dir)\n",
        "    \n",
        "        voice_path_s = os.path.join(data_dir, voice_dir_list[s_label])\n",
        "\n",
        "        count = -1\n",
        "        files = os.listdir(voice_path_s)\n",
        "        for file in files:\n",
        "            if file.count(\"wav\") == 0:\n",
        "                continue\n",
        "\n",
        "            for t_label in range(nb_label):\n",
        "                if (t_label == s_label):\n",
        "                    continue\n",
        "\n",
        "                voice_path_t = os.path.join(data_dir, voice_dir_list[t_label])\n",
        "\n",
        "                wav, _ = librosa.load(os.path.join(voice_path_s, file), sr = sampling_rate, mono = True)\n",
        "                wav = librosa.util.normalize(wav, norm=np.inf, axis=None)\n",
        "                wav = wav_padding(wav = wav, sr = sampling_rate, frame_period = frame_period, multiple = 4)\n",
        "                f0, timeaxis, sp, ap, mc = world_decompose(wav = wav, fs = sampling_rate, frame_period = frame_period)\n",
        "\n",
        "                mc_transposed  = np.array(mc).T\n",
        "\n",
        "                mcep_normalization_params_s = np.load(os.path.join(voice_path_s, \"mcep_\"+voice_dir_list[s_label]+\".npz\"))\n",
        "                mcep_mean_s = mcep_normalization_params_s['mean']\n",
        "                mcep_std_s = mcep_normalization_params_s['std']    \n",
        "                mcep_normalization_params_t = np.load(os.path.join(voice_path_t, \"mcep_\"+voice_dir_list[t_label]+\".npz\"))\n",
        "                mcep_mean_t = mcep_normalization_params_t['mean']\n",
        "                mcep_std_t = mcep_normalization_params_t['std']\n",
        "\n",
        "                mc_norm = (mc_transposed - mcep_mean_s) / mcep_std_s\n",
        "\n",
        "                x = torch.Tensor(mc_norm).view(1, 1, mc_norm.shape[0], mc_norm.shape[1])\n",
        "\n",
        "                label_s_tensor = torch.Tensor(np.array([s_label])).view(1, 1)\n",
        "                label_t_tensor = torch.Tensor(np.array([t_label])).view(1, 1)\n",
        "\n",
        "                x = x.to(device)\n",
        "                label_s_tensor = label_s_tensor.to(device)\n",
        "                label_t_tensor = label_t_tensor.to(device)\n",
        "\n",
        "                mu_enc, logvar_enc = model.encode(x, label_s_tensor)\n",
        "                z_enc = model.reparameterize(mu_enc, logvar_enc)\n",
        "                # x^\n",
        "                mu_dec_t, logvar_dec_t = model.decode(z_enc, label_t_tensor)\n",
        "                z_dec_t = model.reparameterize(mu_dec_t, logvar_dec_t)\n",
        "                if (torch.cuda.is_available()):\n",
        "                    z_dec_t = z_dec_t.data.cpu().numpy().reshape((mc_norm.shape[0], mc_norm.shape[1]))\n",
        "                else:\n",
        "                    z_dec_t = z_dec_t.data.numpy().reshape((mc_norm.shape[0], mc_norm.shape[1]))\n",
        "                # x_\n",
        "                mu_dec_s, logvar_dec_s = model.decode(z_enc, label_s_tensor)\n",
        "                z_dec_s = model.reparameterize(mu_dec_s, logvar_dec_s)\n",
        "                if (torch.cuda.is_available()):\n",
        "                    z_dec_s = z_dec_s.data.cpu().numpy().reshape((mc_norm.shape[0], mc_norm.shape[1]))\n",
        "                else:\n",
        "                    z_dec_s = z_dec_s.data.numpy().reshape((mc_norm.shape[0], mc_norm.shape[1]))\n",
        "\n",
        "                mc_converted_t = z_dec_t * mcep_std_t + mcep_mean_t\n",
        "                mc_converted_t = mc_converted_t.T\n",
        "                mc_converted_t = np.ascontiguousarray(mc_converted_t)\n",
        "                sp_converted_t = world_decode_mc(mc = mc_converted_t, fs = sampling_rate)\n",
        "                mc_converted_s = z_dec_s * mcep_std_s + mcep_mean_s\n",
        "                mc_converted_s = mc_converted_s.T\n",
        "                mc_converted_s = np.ascontiguousarray(mc_converted_s)\n",
        "                sp_converted_s = world_decode_mc(mc = mc_converted_s, fs = sampling_rate)\n",
        "\n",
        "                sp_gained = np.multiply(sp, np.divide(sp_converted_t, sp_converted_s))\n",
        "\n",
        "                logf0s_normalization_params_s = np.load(os.path.join(voice_path_s, \"log_f0_\"+voice_dir_list[s_label]+\".npz\"))\n",
        "                logf0s_mean_s = logf0s_normalization_params_s['mean']\n",
        "                logf0s_std_s = logf0s_normalization_params_s['std']\n",
        "                logf0s_normalization_params_t = np.load(os.path.join(voice_path_t, \"log_f0_\"+voice_dir_list[t_label]+\".npz\"))\n",
        "                logf0s_mean_t = logf0s_normalization_params_t['mean']\n",
        "                logf0s_std_t = logf0s_normalization_params_t['std']\n",
        "\n",
        "                f0_converted = pitch_conversion(f0 = f0, mean_log_src = logf0s_mean_s, std_log_src = logf0s_std_s, mean_log_target = logf0s_mean_t, std_log_target = logf0s_std_t)\n",
        "\n",
        "                wav_transformed = world_speech_synthesis(f0 = f0_converted, sp = sp_gained, ap = ap, fs = sampling_rate, frame_period = frame_period)\n",
        "                #librosa.output.write_wav(os.path.join(output_label_dir, voice_dir_list[s_label]+\"_to_\"+voice_dir_list[t_label]+\"_[\"+file+\"].wav\"), wav_transformed, sampling_rate)\n",
        "                sf.write(os.path.join(output_label_dir, voice_dir_list[s_label]+\"_to_\"+voice_dir_list[t_label]+\"_[\"+file+\"].wav\"),\n",
        "                     wav_transformed, \n",
        "                     sampling_rate)\n",
        "                \n",
        "                wav_source = world_speech_synthesis(f0 = f0_converted, sp = sp, ap = ap, fs = sampling_rate, frame_period = frame_period)\n",
        "                #librosa.output.write_wav(os.path.join(output_label_dir, voice_dir_list[s_label]+\"_to_\"+voice_dir_list[t_label]+\"_[\"+file+\"]_nonconv.wav\"), wav_source, sampling_rate)\n",
        "                sf.write(os.path.join(output_label_dir, voice_dir_list[s_label]+\"_to_\"+voice_dir_list[t_label]+\"_[\"+file+\"]_nonconv.wav\"),\n",
        "                     wav_source, \n",
        "                     sampling_rate)\n",
        "\n",
        "            count += 1\n",
        "            if (count % 10 == 0):\n",
        "                print(\"{} ({}/{}) : {:.1f} % is done...\".format(voice_dir_list[s_label], str(s_label+1), str(nb_label), count*100/len(files)))\n",
        "    print(\"Finish.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2gNzmCdL4Qkf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Conversion Start.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/bernardotorres/Dropbox/MVA/Courses/Audio indexing/acvae-vc/preprocess.py:188: RuntimeWarning: divide by zero encountered in log\n",
            "  f0_converted = np.exp((np.log(f0) - mean_log_src) / std_log_src * std_log_target + mean_log_target)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SF1 (1/4) : 0.0 % is done...\n",
            "SF1 (1/4) : 6.1 % is done...\n",
            "SF1 (1/4) : 12.2 % is done...\n",
            "SF1 (1/4) : 18.3 % is done...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-262489a40d8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mconv_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-1742237888b7>\u001b[0m in \u001b[0;36mconv_all\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworld_decompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_period\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mmc_transposed\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Dropbox/MVA/Courses/Audio indexing/acvae-vc/preprocess.py\u001b[0m in \u001b[0;36mworld_decompose\u001b[0;34m(wav, fs, frame_period, num_mcep)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Decompose speech signal into f0, spectral envelope and aperiodicity using WORLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeaxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mharvest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0_floor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m71.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0_ceil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m800.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheaptrick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md4c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model = ACVAE(nb_label=nb_label,lambda_p=lambda_p,lambda_s=lambda_s).to(device)\n",
        "model = model_load(model, model_dir, model_name)\n",
        "conv_all(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ACVAE-AC",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
