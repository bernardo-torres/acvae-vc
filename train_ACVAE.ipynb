{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import pyworld\n",
    "import librosa\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocess import *\n",
    "from model import *\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model_lambda70_f2f3m1m2\"\n",
    "model_dir = \"./model/\" + model_name\n",
    "\n",
    "data_dir = \"./data/vcc2016_training\"\n",
    "voice_dir_list = [\"SF1\", \"SF2\", \"SM1\", \"SM2\"]\n",
    "\n",
    "output_dir = \"./converted_voices/test/\" + model_name + \"_training_progress\"\n",
    "figure_dir = \"./figure/\" + model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_p = 70\n",
    "lambda_s = 70\n",
    "nb_label = len(voice_dir_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "batch_size = 4\n",
    "learning_rate =1e-3\n",
    "learning_rate_ = 1e-4\n",
    "learning_rate__ = 1e-5\n",
    "learning_rate___ = 1e-6\n",
    "sampling_rate = 16000\n",
    "num_envelope  = 36\n",
    "num_mcep = 36\n",
    "frame_period = 5.0\n",
    "n_frames = 1024 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess: SF1\n",
      "Preprocessing Data...\n",
      "Data Loading...\n",
      "Extracting f0 and mcep...\n",
      "Saving f0 Data...\n",
      "Saving mcep Data...\n",
      "Preprocessing Done.\n",
      "Time Elapsed for Data Preprocessing: 00:04:23\n"
     ]
    }
   ],
   "source": [
    "for v in voice_dir_list:\n",
    "    if \"log_f0_\"+v+\".npz\" in  os.listdir(os.path.join(data_dir, v)):\n",
    "        continue\n",
    "    print(\"Preprocess: \" + v)\n",
    "    preprocess_voice(os.path.join(data_dir, v), v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save(model, model_dir, model_name):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    torch.save(model.state_dict(), os.path.join(model_dir, model_name))\n",
    "    \n",
    "def model_load(model_dir, model_name):\n",
    "    model = ACVAE(nb_label=nb_label,lambda_p=lambda_p,lambda_s=lambda_s)\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir, model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(losses, epoch):        \n",
    "    if not os.path.exists(figure_dir):\n",
    "            os.makedirs(figure_dir)\n",
    "    losses = np.array(losses)\n",
    "    losses = losses.reshape(-1, 4)\n",
    "    x = np.linspace(0, len(losses), len(losses))\n",
    "    losses_label = (\"L1\", \"KLD\", \"AC_p\", \"AC_s\")\n",
    "    plt.figure()\n",
    "    plt.plot(x, losses[:,0], label=losses_label[0])\n",
    "    plt.plot(x, losses[:,1], label=losses_label[1])\n",
    "    plt.plot(x, losses[:,2], label=losses_label[2])\n",
    "    plt.plot(x, losses[:,3], label=losses_label[3])\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc='upper right', borderaxespad=0)\n",
    "    plt.savefig(figure_dir + \"/\" + \"epoch_{:05}\".format(epoch) + \".png\")\n",
    "    plt.savefig(figure_dir + \"/\" + \"result.png\")\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.plot(x, losses[:,2], label=losses_label[2])\n",
    "    plt.plot(x, losses[:,3], label=losses_label[3])\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc='upper right', borderaxespad=0)\n",
    "    plt.savefig(figure_dir + \"/\" + \"epoch_{:05}_AC\".format(epoch) + \".png\")\n",
    "    plt.savefig(figure_dir + \"/\" + \"result_AC.png\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(x, losses[:,0], label=losses_label[0])\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc='upper right', borderaxespad=0)\n",
    "    plt.savefig(figure_dir + \"/\" + \"epoch_{:05}_L1\".format(epoch) + \".png\")\n",
    "    plt.savefig(figure_dir + \"/\" + \"result_L1.png\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(x, losses[:,1], label=losses_label[1])\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc='upper right', borderaxespad=0)\n",
    "    plt.savefig(figure_dir + \"/\" + \"epoch_{:05}_KLD\".format(epoch) + \".png\")\n",
    "    plt.savefig(figure_dir + \"/\" + \"result_KLD.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 512\n",
    "def data_load(batchsize = 1, s = -1, t = -1):\n",
    "    x = []\n",
    "    label = []\n",
    "    for i in range(batchsize):\n",
    "        if (s == -1):\n",
    "            label_num = np.random.randint(nb_label)\n",
    "        else:\n",
    "            label_num = s\n",
    "        voice_path = os.path.join(data_dir, voice_dir_list[label_num])\n",
    "        files = os.listdir(voice_path)\n",
    "        \n",
    "        frames = 0\n",
    "        while frames < n_frames:\n",
    "            \n",
    "            file = \"\"\n",
    "            while file.count(\"wav\") == 0:\n",
    "                file = np.random.choice(files)\n",
    "            wav, _ = librosa.load(os.path.join(voice_path, file), sr = sampling_rate, mono = True)\n",
    "            wav = librosa.util.normalize(wav, norm=np.inf, axis=None)\n",
    "            wav = wav_padding(wav = wav, sr = sampling_rate, frame_period = frame_period, multiple = 4)\n",
    "            f0, timeaxis, sp, ap, mc = world_decompose(wav = wav, fs = sampling_rate, frame_period = frame_period, num_mcep = num_mcep)\n",
    "            \n",
    "            mc_transposed  = np.array(mc).T\n",
    "            frames = np.shape(mc_transposed)[1]\n",
    "            print(frames, os.path.join(voice_path, file))\n",
    "            \n",
    "        mcep_normalization_params = np.load(os.path.join(voice_path, \"mcep_\"+voice_dir_list[label_num]+\".npz\"))\n",
    "        mcep_mean = mcep_normalization_params['mean']\n",
    "        mcep_std = mcep_normalization_params['std']\n",
    "        mc_norm = (mc_transposed  - mcep_mean) / mcep_std\n",
    "            \n",
    "        start_ = np.random.randint(frames - n_frames + 1)\n",
    "        end_ = start_ + n_frames\n",
    "            \n",
    "        x.append(mc_norm[:,start_:end_])\n",
    "        label.append(label_num)\n",
    "\n",
    "    return torch.Tensor(x).view(batchsize, 1, num_mcep, n_frames), torch.Tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664 ./data/vcc2016_training/SF1/100022.wav\n",
      "608 ./data/vcc2016_training/SF2/100109.wav\n",
      "800 ./data/vcc2016_training/SM1/100050.wav\n",
      "1824 ./data/vcc2016_training/SF2/100096.wav\n",
      "544 ./data/vcc2016_training/SF2/100137.wav\n",
      "576 ./data/vcc2016_training/SF2/100068.wav\n",
      "832 ./data/vcc2016_training/SF1/100083.wav\n",
      "1024 ./data/vcc2016_training/SM1/100040.wav\n",
      "torch.Size([8, 1, 36, 512]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "a = data_load(8)\n",
    "print(a[0].shape, a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conv(model, epoch):\n",
    "    print(\"Test\")\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    output_epoch_dir = os.path.join(output_dir, \"epoch_{:05}\".format(epoch))\n",
    "    if not os.path.exists(output_epoch_dir):\n",
    "        os.makedirs(output_epoch_dir)\n",
    "    \n",
    "    for s_label in range(nb_label):\n",
    "    \n",
    "        voice_path_s = os.path.join(data_dir, voice_dir_list[s_label])\n",
    "\n",
    "        files = os.listdir(voice_path_s)\n",
    "        file = \"\"\n",
    "        while file.count(\"wav\") == 0:\n",
    "            file = np.random.choice(files)\n",
    "        \n",
    "        print(\"Source File:\" + file)\n",
    "\n",
    "        for t_label in range(nb_label):\n",
    "\n",
    "            if (t_label == s_label):\n",
    "                continue\n",
    "\n",
    "            voice_path_t = os.path.join(data_dir, voice_dir_list[t_label])\n",
    "\n",
    "            wav, _ = librosa.load(os.path.join(voice_path_s, file), sr = sampling_rate, mono = True)\n",
    "            wav = librosa.util.normalize(wav, norm=np.inf, axis=None)\n",
    "            wav = wav_padding(wav = wav, sr = sampling_rate, frame_period = frame_period, multiple = 4)\n",
    "            f0, timeaxis, sp, ap, mc = world_decompose(wav = wav, fs = sampling_rate, frame_period = frame_period)\n",
    "\n",
    "            mc_transposed  = np.array(mc).T\n",
    "\n",
    "            mcep_normalization_params_s = np.load(os.path.join(voice_path_s, \"mcep_\"+voice_dir_list[s_label]+\".npz\"))\n",
    "            mcep_mean_s = mcep_normalization_params_s['mean']\n",
    "            mcep_std_s = mcep_normalization_params_s['std']    \n",
    "            mcep_normalization_params_t = np.load(os.path.join(voice_path_t, \"mcep_\"+voice_dir_list[t_label]+\".npz\"))\n",
    "            mcep_mean_t = mcep_normalization_params_t['mean']\n",
    "            mcep_std_t = mcep_normalization_params_t['std']\n",
    "\n",
    "            mc_norm = (mc_transposed - mcep_mean_s) / mcep_std_s\n",
    "\n",
    "            x = torch.Tensor(mc_norm).view(1, 1, mc_norm.shape[0], mc_norm.shape[1])\n",
    "\n",
    "            label_s_tensor = torch.Tensor(np.array([s_label])).view(1, 1)\n",
    "            label_t_tensor = torch.Tensor(np.array([t_label])).view(1, 1)\n",
    "\n",
    "            x = x.to(device)\n",
    "            label_s_tensor = label_s_tensor.to(device)\n",
    "            label_t_tensor = label_t_tensor.to(device)\n",
    "\n",
    "            mu_enc, logvar_enc = model.encode(x, label_s_tensor)\n",
    "            z_enc = model.reparameterize(mu_enc, logvar_enc)\n",
    "            # x^\n",
    "            mu_dec_t, logvar_dec_t = model.decode(z_enc, label_t_tensor)\n",
    "            z_dec_t = model.reparameterize(mu_dec_t, logvar_dec_t)\n",
    "            if (torch.cuda.is_available()):\n",
    "                z_dec_t = z_dec_t.data.cpu().numpy().reshape((mc_norm.shape[0], mc_norm.shape[1]))\n",
    "            else:\n",
    "                z_dec_t = z_dec_t.data.numpy().reshape((mc_norm.shape[0], mc_norm.shape[1]))\n",
    "            # x_\n",
    "            mu_dec_s, logvar_dec_s = model.decode(z_enc, label_s_tensor)\n",
    "            z_dec_s = model.reparameterize(mu_dec_s, logvar_dec_s)\n",
    "            if (torch.cuda.is_available()):\n",
    "                z_dec_s = z_dec_s.data.cpu().numpy().reshape((mc_norm.shape[0], mc_norm.shape[1]))\n",
    "            else:\n",
    "                z_dec_s = z_dec_s.data.numpy().reshape((mc_norm.shape[0], mc_norm.shape[1]))\n",
    "\n",
    "            mc_converted_t = z_dec_t * mcep_std_t + mcep_mean_t\n",
    "            mc_converted_t = mc_converted_t.T\n",
    "            mc_converted_t = np.ascontiguousarray(mc_converted_t)\n",
    "            sp_converted_t = world_decode_mc(mc = mc_converted_t, fs = sampling_rate)\n",
    "            mc_converted_s = z_dec_s * mcep_std_s + mcep_mean_s\n",
    "            mc_converted_s = mc_converted_s.T\n",
    "            mc_converted_s = np.ascontiguousarray(mc_converted_s)\n",
    "            sp_converted_s = world_decode_mc(mc = mc_converted_s, fs = sampling_rate)\n",
    "\n",
    "            sp_gained = np.multiply(sp, np.divide(sp_converted_t, sp_converted_s))\n",
    "\n",
    "            logf0s_normalization_params_s = np.load(os.path.join(voice_path_s, \"log_f0_\"+voice_dir_list[s_label]+\".npz\"))\n",
    "            logf0s_mean_s = logf0s_normalization_params_s['mean']\n",
    "            logf0s_std_s = logf0s_normalization_params_s['std']\n",
    "            logf0s_normalization_params_t = np.load(os.path.join(voice_path_t, \"log_f0_\"+voice_dir_list[t_label]+\".npz\"))\n",
    "            logf0s_mean_t = logf0s_normalization_params_t['mean']\n",
    "            logf0s_std_t = logf0s_normalization_params_t['std']\n",
    "\n",
    "            f0_converted = pitch_conversion(f0 = f0, mean_log_src = logf0s_mean_s, std_log_src = logf0s_std_s, mean_log_target = logf0s_mean_t, std_log_target = logf0s_std_t)\n",
    "            \n",
    "            wav_transformed = world_speech_synthesis(f0 = f0_converted, sp = sp_gained, ap = ap, fs = sampling_rate, frame_period = frame_period)\n",
    "            librosa.output.write_wav(os.path.join(output_epoch_dir, voice_dir_list[s_label]+\"_to_\"+voice_dir_list[t_label]+\"_[\"+file+\"].wav\"), wav_transformed, sampling_rate)\n",
    "            wav_source = world_speech_synthesis(f0 = f0_converted, sp = sp, ap = ap, fs = sampling_rate, frame_period = frame_period)\n",
    "            librosa.output.write_wav(os.path.join(output_epoch_dir, voice_dir_list[s_label]+\"_to_\"+voice_dir_list[t_label]+\"_[\"+file+\"]_nonconv.wav\"), wav_source, sampling_rate)\n",
    "\n",
    "            print(\"Converted: \" + voice_dir_list[s_label] + \" -> \" + voice_dir_list[t_label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-b90999c39410>:37: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return torch.Tensor(x).view(batchsize, 1, num_mcep, n_frames), torch.Tensor(label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed for one epoch: 00:00:13\n",
      "Epoch: 2\n",
      "Time Elapsed for one epoch: 00:00:15\n",
      "Epoch: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-04d90746115f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b90999c39410>\u001b[0m in \u001b[0;36mdata_load\u001b[0;34m(batchsize, s, t)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworld_decompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mcep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_mcep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mmc_transposed\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/MVA/Courses/Audio indexing/acvae-vc/preprocess.py\u001b[0m in \u001b[0;36mworld_decompose\u001b[0;34m(wav, fs, frame_period, num_mcep)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeaxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mharvest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_period\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0_floor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m71.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0_ceil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m800.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheaptrick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md4c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpysptk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmcepalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = ACVAE(nb_label=nb_label,lambda_p=lambda_p,lambda_s=lambda_s).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch += 1\n",
    "    \n",
    "    if (epoch == 3000):\n",
    "        learning_rate = learning_rate_ \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "    if (epoch == 6000):\n",
    "        learning_rate = learning_rate__\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "    if (epoch == 8000):\n",
    "        learning_rate = learning_rate___\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    print('Epoch: %d' % epoch)\n",
    "\n",
    "    x_, label_ = data_load(batch_size)\n",
    "    optimizer.zero_grad()\n",
    "    loss, loss_list = model.calc_loss(x_, label_)\n",
    "    loss.backward()\n",
    "    losses.append(loss_list)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        test_conv(model, epoch)\n",
    "    if epoch % 100 == 0:\n",
    "        model_save(model, model_dir, model_name)\n",
    "    if epoch % 2000 == 0:\n",
    "        model_save(model, model_dir, model_name + \"_\" + str(epoch))\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        save_figure(losses, epoch)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Time Elapsed for one epoch: %02d:%02d:%02d' % (elapsed_time // 3600, (elapsed_time % 3600 // 60), (elapsed_time % 60 // 1)))\n",
    "\n",
    "model_save(model, model_dir, model_name)\n",
    "\n",
    "save_figure(losses, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b= data_load(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(a[0,0].flatten()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
